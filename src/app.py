from io import BytesIO

import streamlit as st
from importlib import import_module

import_module("torch") # needed to ensure safe DLL loading sequence on windows

from evaluator import Evaluator, EvaluatorResult
from pipeline import (
    EmotionPredictionModel,
    Pipeline,
    PipelineResult,
    SpeechToTextModel,
    TextToSpeechModel,
)
from response_generator import LlmResponseGenerator, SYSTEM_PROMPT_DICT
from emotion_prediction_model import EMOTION_PREDICTION_MODEL_DICT
from utils import get_device

device = get_device()
if device == "cpu":
    st.warning("Using CPU for inference")

should_show_intermediate_steps: bool = st.toggle(
    label="Show intermediate steps",
    help="Turn on to show the outputs of intermediate steps of the pipeline",
)
should_evaluate: bool = st.toggle(
    label="Evaluate results against metrics",
    help="Turn on to evaluate results against standardized metrics\n\n[!WARNING]\nFor now this works only for RAVDESS filenames.",
)
_aci_mode = st.segmented_control(
    label="ACI Mode",
    options=SYSTEM_PROMPT_DICT.keys(),
    selection_mode='single',
    default='Categorical',
    help="Set ACI mode",
)
_llm_instruction_level = st.segmented_control(
    label="Set instruction level given to LLM",
    options=SYSTEM_PROMPT_DICT["Categorical"].keys(),
    selection_mode='single',
    default="Full",
    help="Set LLM instruction level",
)
_temperature = st.slider(
    label="Temperature",
    min_value=0.0,
    max_value=2.0,
    value=0.7,
    step=0.02,
    help="Set temperature for LLM.\nIt controls the randomness of text generated by LLM during inference.",
)


@st.cache_resource(show_spinner="Loading Speech to Text Model")
def get_speech_to_text_model() -> SpeechToTextModel:
    return SpeechToTextModel()


@st.cache_resource(show_spinner="Loading Emotion Prediction Model")
def get_emotion_prediction_model(aci_mode: str) -> EmotionPredictionModel:
    if 'emotion_prediction_model_dict' not in st.session_state:
        st.session_state['emotion_prediction_model_dict'] = {}
    model_dict: dict[str, EmotionPredictionModel] = st.session_state['emotion_prediction_model_dict']
    if aci_mode not in model_dict:
        required_model_class = EMOTION_PREDICTION_MODEL_DICT[aci_mode]
        model_dict[aci_mode] = required_model_class(device=device)
    return model_dict[aci_mode]


@st.cache_resource(show_spinner="Loading Llm Response Generator")
def get_llm_response_generator(llm_instruction_level: str, aci_mode: str, temperature: float) -> LlmResponseGenerator:
    return LlmResponseGenerator(
        provider="google_genai",
        model="gemini-2.5-flash",
        aci_mode=aci_mode,
        llm_instruction_level=llm_instruction_level,
        temperature=temperature
    )


@st.cache_resource(show_spinner="Loading Text to Speech Model")
def get_text_to_speech_model() -> TextToSpeechModel:
    return TextToSpeechModel(device=device)


@st.cache_resource(show_spinner='Initializing pipeline')
def get_pipeline(llm_instruction_level: str, aci_mode: str, temperature: float) -> Pipeline:
    return Pipeline(
        speech_to_text_model=get_speech_to_text_model(),
        emotion_prediction_model=get_emotion_prediction_model(aci_mode=aci_mode),
        llm_response_generator=get_llm_response_generator(
            llm_instruction_level=llm_instruction_level,
            aci_mode=aci_mode,
            temperature=temperature,
        ),
        text_to_speech_model=get_text_to_speech_model(),
    )


@st.cache_resource(show_spinner="Initializing response evaluator")
def get_evaluator() -> Evaluator:
    return Evaluator()


pipeline = get_pipeline(
    llm_instruction_level=_llm_instruction_level,
    aci_mode=_aci_mode,
    temperature=_temperature,
)


@st.cache_resource(show_spinner="Processing your audio... ðŸŽ§")
def call_pipeline_with_cache(uploaded_file: BytesIO) -> PipelineResult:
    return pipeline(uploaded_file)


@st.cache_resource(show_spinner="Evaluating LLM output")
def get_evaluation_result(filename: str, response_text: str) -> EvaluatorResult:
    evaluator = get_evaluator()
    return evaluator(filename=filename, response_text=response_text)


_uploaded_file = st.file_uploader(
    "Upload an audio file to process", type=["mp3", "wav", "m4a", "ogg", ".flac"]
)

if _uploaded_file is not None:
    st.audio(_uploaded_file, format="audio/wav")

    result = call_pipeline_with_cache(_uploaded_file)

    if should_show_intermediate_steps:
        st.subheader("Speech to Text Transcription:")
        st.write(result.speech_to_text_transcript)

        st.subheader("Speech Emotion Recognition Results")
        predicted_emotions_dict = result.predicted_emotional_state.encode_as_dict()
        st.table(predicted_emotions_dict)
        st.bar_chart(predicted_emotions_dict)

        st.subheader("LLM Response Text")
        st.write(result.llm_response)

    st.subheader("Response")
    st.audio(result.audio_output, sample_rate=result.audio_output_sample_rate)

    if should_evaluate:
        evaluator_result = get_evaluation_result(
            filename=_uploaded_file.name,
            response_text=result.llm_response,
        )

        st.subheader("Evaluation Results")
        st.table(evaluator_result.as_flat_dict())
